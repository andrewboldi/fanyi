{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e42daab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting git+https://github.com/bigscience-workshop/petals\n",
      "  Cloning https://github.com/bigscience-workshop/petals to /tmp/pip-req-build-zzg9wg8v\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/bigscience-workshop/petals /tmp/pip-req-build-zzg9wg8v\n",
      "  Resolved https://github.com/bigscience-workshop/petals to commit ae19b650959fadd837f4db799f9ec35011199506\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.12 in /home/andrew/.local/lib/python3.10/site-packages (from petals==2.2.0) (2.2.0.dev20230920+cu121)\n",
      "Collecting bitsandbytes==0.41.1 (from petals==2.2.0)\n",
      "  Obtaining dependency information for bitsandbytes==0.41.1 from https://files.pythonhosted.org/packages/1e/2c/af22cd797fc368a9f098ed03015730e6568b884fe67f9940793d944a4b7b/bitsandbytes-0.41.1-py3-none-any.whl.metadata\n",
      "  Downloading bitsandbytes-0.41.1-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting accelerate>=0.22.0 (from petals==2.2.0)\n",
      "  Obtaining dependency information for accelerate>=0.22.0 from https://files.pythonhosted.org/packages/d9/92/2d3aecf9f4a192968035880be3e2fc8b48d541c7128f7c936f430d6f96da/accelerate-0.23.0-py3-none-any.whl.metadata\n",
      "  Downloading accelerate-0.23.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting huggingface-hub<1.0.0,>=0.11.1 (from petals==2.2.0)\n",
      "  Obtaining dependency information for huggingface-hub<1.0.0,>=0.11.1 from https://files.pythonhosted.org/packages/ef/b5/b6107bd65fa4c96fdf00e4733e2fe5729bb9e5e09997f63074bb43d3ab28/huggingface_hub-0.18.0-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tokenizers>=0.13.3 (from petals==2.2.0)\n",
      "  Obtaining dependency information for tokenizers>=0.13.3 from https://files.pythonhosted.org/packages/a7/7b/c1f643eb086b6c5c33eef0c3752e37624bd23e4cbc9f1332748f1c6252d1/tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting transformers<5.0.0,>=4.32.0 (from petals==2.2.0)\n",
      "  Obtaining dependency information for transformers<5.0.0,>=4.32.0 from https://files.pythonhosted.org/packages/1a/d1/3bba59606141ae808017f6fde91453882f931957f125009417b87a281067/transformers-4.34.0-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.34.0-py3-none-any.whl.metadata (121 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting speedtest-cli==2.1.3 (from petals==2.2.0)\n",
      "  Downloading speedtest_cli-2.1.3-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pydantic<2.0,>=1.10 (from petals==2.2.0)\n",
      "  Obtaining dependency information for pydantic<2.0,>=1.10 from https://files.pythonhosted.org/packages/e0/2f/d6f17f8385d718233bcae893d27525443d41201c938b68a4af3d591a33e4/pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hivemind==1.1.10.post2 (from petals==2.2.0)\n",
      "  Downloading hivemind-1.1.10.post2.tar.gz (242 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensor-parallel==1.0.23 (from petals==2.2.0)\n",
      "  Downloading tensor_parallel-1.0.23-py3-none-any.whl (25 kB)\n",
      "Collecting humanfriendly (from petals==2.2.0)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout>=4.0.2 (from petals==2.2.0)\n",
      "  Obtaining dependency information for async-timeout>=4.0.2 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/andrew/.local/lib/python3.10/site-packages (from petals==2.2.0) (23.1)\n",
      "Collecting sentencepiece>=0.1.99 (from petals==2.2.0)\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting peft>=0.5.0 (from petals==2.2.0)\n",
      "  Obtaining dependency information for peft>=0.5.0 from https://files.pythonhosted.org/packages/37/1a/8d20e8704da9fa070eb909265584b960da57be1d833d550c59f50906dc5c/peft-0.5.0-py3-none-any.whl.metadata\n",
      "  Downloading peft-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting safetensors>=0.3.1 (from petals==2.2.0)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/20/4e/878b080dbda92666233ec6f316a53969edcb58eab1aa399a64d0521cf953/safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting Dijkstar>=2.6.0 (from petals==2.2.0)\n",
      "  Downloading Dijkstar-2.6.0-py3-none-any.whl (12 kB)\n",
      "Collecting cpufeature>=0.2.0 (from petals==2.2.0)\n",
      "  Downloading cpufeature-0.2.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from hivemind==1.1.10.post2->petals==2.2.0) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/andrew/.local/lib/python3.10/site-packages (from hivemind==1.1.10.post2->petals==2.2.0) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /usr/lib/python3/dist-packages (from hivemind==1.1.10.post2->petals==2.2.0) (1.8.0)\n",
      "Collecting prefetch-generator>=1.0.1 (from hivemind==1.1.10.post2->petals==2.2.0)\n",
      "  Downloading prefetch_generator-1.0.3.tar.gz (4.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: msgpack>=0.5.6 in /usr/lib/python3/dist-packages (from hivemind==1.1.10.post2->petals==2.2.0) (1.0.3)\n",
      "Requirement already satisfied: sortedcontainers in /home/andrew/.local/lib/python3.10/site-packages (from hivemind==1.1.10.post2->petals==2.2.0) (2.4.0)\n",
      "Collecting uvloop>=0.14.0 (from hivemind==1.1.10.post2->petals==2.2.0)\n",
      "  Obtaining dependency information for uvloop>=0.14.0 from https://files.pythonhosted.org/packages/8b/12/6c12bc92495cd779d6d6d3bb1bc3dca54b59cb1c78b4777e9c6bc3f39998/uvloop-0.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading uvloop-0.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting grpcio-tools>=1.33.2 (from hivemind==1.1.10.post2->petals==2.2.0)\n",
      "  Obtaining dependency information for grpcio-tools>=1.33.2 from https://files.pythonhosted.org/packages/5f/a7/4bdee1539e0bc018a9c908be8b1d409fc23ebc325585070231aab31e7657/grpcio_tools-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading grpcio_tools-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /usr/lib/python3/dist-packages (from hivemind==1.1.10.post2->petals==2.2.0) (3.12.4)\n",
      "Collecting configargparse>=1.2.3 (from hivemind==1.1.10.post2->petals==2.2.0)\n",
      "  Obtaining dependency information for configargparse>=1.2.3 from https://files.pythonhosted.org/packages/6f/b3/b4ac838711fd74a2b4e6f746703cf9dd2cf5462d17dac07e349234e21b97/ConfigArgParse-1.7-py3-none-any.whl.metadata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading ConfigArgParse-1.7-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting multiaddr>=0.0.9 (from hivemind==1.1.10.post2->petals==2.2.0)\n",
      "  Downloading multiaddr-0.0.9-py2.py3-none-any.whl (16 kB)\n",
      "Collecting pymultihash>=0.8.2 (from hivemind==1.1.10.post2->petals==2.2.0)\n",
      "  Downloading pymultihash-0.8.2-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: cryptography>=3.4.6 in /home/andrew/.local/lib/python3.10/site-packages (from hivemind==1.1.10.post2->petals==2.2.0) (41.0.4)\n",
      "Requirement already satisfied: psutil in /home/andrew/.local/lib/python3.10/site-packages (from accelerate>=0.22.0->petals==2.2.0) (5.9.5)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from Dijkstar>=2.6.0->petals==2.2.0) (1.16.0)\n",
      "Requirement already satisfied: filelock in /home/andrew/.local/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.1->petals==2.2.0) (3.9.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0.0,>=0.11.1->petals==2.2.0)\n",
      "  Obtaining dependency information for fsspec>=2023.5.0 from https://files.pythonhosted.org/packages/fe/d3/e1aa96437d944fbb9cc95d0316e25583886e9cd9e6adc07baad943524eda/fsspec-2023.9.2-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.9.2-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: requests in /home/andrew/.local/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.1->petals==2.2.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/andrew/.local/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.1->petals==2.2.0) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/andrew/.local/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.1->petals==2.2.0) (4.8.0)\n",
      "Collecting huggingface-hub<1.0.0,>=0.11.1 (from petals==2.2.0)\n",
      "  Obtaining dependency information for huggingface-hub<1.0.0,>=0.11.1 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: fsspec in /home/andrew/.local/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.1->petals==2.2.0) (2023.4.0)\n",
      "Requirement already satisfied: sympy in /usr/lib/python3/dist-packages (from torch>=1.12->petals==2.2.0) (1.9)\n",
      "Requirement already satisfied: networkx in /home/andrew/.local/lib/python3.10/site-packages (from torch>=1.12->petals==2.2.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=1.12->petals==2.2.0) (3.0.3)\n",
      "Requirement already satisfied: pytorch-triton==2.1.0+6e4932cda8 in /home/andrew/.local/lib/python3.10/site-packages (from torch>=1.12->petals==2.2.0) (2.1.0+6e4932cda8)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/andrew/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->petals==2.2.0) (2023.10.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/andrew/.local/lib/python3.10/site-packages (from cryptography>=3.4.6->hivemind==1.1.10.post2->petals==2.2.0) (1.15.1)\n",
      "Collecting protobuf>=3.12.2 (from hivemind==1.1.10.post2->petals==2.2.0)\n",
      "  Obtaining dependency information for protobuf>=3.12.2 from https://files.pythonhosted.org/packages/c8/2c/03046cac73f46bfe98fc846ef629cf4f84c2f59258216aa2cc0d22bfca8f/protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\n",
      "Collecting grpcio>=1.59.0 (from grpcio-tools>=1.33.2->hivemind==1.1.10.post2->petals==2.2.0)\n",
      "  Obtaining dependency information for grpcio>=1.59.0 from https://files.pythonhosted.org/packages/20/7f/e76618521aa9d33c6c1c9c3473f866da521678aa6ea2f4df3a896757748c/grpcio-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading grpcio-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: setuptools in /home/andrew/.local/lib/python3.10/site-packages (from grpcio-tools>=1.33.2->hivemind==1.1.10.post2->petals==2.2.0) (68.2.2)\n",
      "Collecting varint (from multiaddr>=0.0.9->hivemind==1.1.10.post2->petals==2.2.0)\n",
      "  Downloading varint-1.0.2.tar.gz (1.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting base58 (from multiaddr>=0.0.9->hivemind==1.1.10.post2->petals==2.2.0)\n",
      "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
      "Collecting netaddr (from multiaddr>=0.0.9->hivemind==1.1.10.post2->petals==2.2.0)\n",
      "  Obtaining dependency information for netaddr from https://files.pythonhosted.org/packages/68/4a/aa1a61a8cdfc533020012594b006bbb3e7680326e1c247f63e711be5ab76/netaddr-0.9.0-py3-none-any.whl.metadata\n",
      "  Downloading netaddr-0.9.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/andrew/.local/lib/python3.10/site-packages (from requests->huggingface-hub<1.0.0,>=0.11.1->petals==2.2.0) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub<1.0.0,>=0.11.1->petals==2.2.0) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface-hub<1.0.0,>=0.11.1->petals==2.2.0) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/andrew/.local/lib/python3.10/site-packages (from requests->huggingface-hub<1.0.0,>=0.11.1->petals==2.2.0) (2023.7.22)\n",
      "Requirement already satisfied: pycparser in /home/andrew/.local/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.4.6->hivemind==1.1.10.post2->petals==2.2.0) (2.21)\n",
      "Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading peft-0.5.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0mm\n",
      "\u001b[?25hDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
      "Downloading grpcio_tools-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl (311 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvloop-0.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading grpcio-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading netaddr-0.9.0-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: petals, hivemind, prefetch-generator, varint\n",
      "  Building wheel for petals (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for petals: filename=petals-2.2.0-py3-none-any.whl size=114221 sha256=f5a2b1c401027f9b0ec42454ae5cf5c2c00ef4df6ce692ce27826198b8b0cf23\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-q86ixw4i/wheels/98/38/8f/58b4f2d1d3762b120b9747915042f7498efd33a7d7475228ce\n",
      "  Building wheel for hivemind (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hivemind: filename=hivemind-1.1.10.post2-py3-none-any.whl size=9011440 sha256=f40e62b5c713ff958fe2dd743b05f6bb99c3917fd80b8c9cc277be4a039822c2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-q86ixw4i/wheels/53/98/e3/d51ae54ae8302c5a1872fae2e85a8e8291651af19f550965c1\n",
      "  Building wheel for prefetch-generator (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for prefetch-generator: filename=prefetch_generator-1.0.3-py3-none-any.whl size=4758 sha256=6e3ffee4d0375063649bd46e50c43e1ba82a2ea32ebf3593f9822b32601b7e24\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-q86ixw4i/wheels/65/65/44/ed059bc23eeda93cfb19b58113423882cd9febae7ebf3f9ddf\n",
      "  Building wheel for varint (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for varint: filename=varint-1.0.2-py3-none-any.whl size=1961 sha256=75d61113c69d0e3d50b9e42b48547ee55852e4a25dbb0432ff52fe7712051d5c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-q86ixw4i/wheels/39/48/5e/33919c52a2a695a512ca394a5308dd12626a40bbcd288de814\n",
      "Successfully built petals hivemind prefetch-generator varint\n",
      "Installing collected packages: varint, speedtest-cli, sentencepiece, pymultihash, prefetch-generator, netaddr, cpufeature, bitsandbytes, uvloop, safetensors, pydantic, protobuf, humanfriendly, grpcio, Dijkstar, configargparse, base58, async-timeout, multiaddr, huggingface-hub, grpcio-tools, tokenizers, hivemind, accelerate, transformers, tensor-parallel, peft, petals\n",
      "Successfully installed Dijkstar-2.6.0 accelerate-0.23.0 async-timeout-4.0.3 base58-2.1.1 bitsandbytes-0.41.1 configargparse-1.7 cpufeature-0.2.1 grpcio-1.59.0 grpcio-tools-1.59.0 hivemind-1.1.10.post2 huggingface-hub-0.17.3 humanfriendly-10.0 multiaddr-0.0.9 netaddr-0.9.0 peft-0.5.0 petals-2.2.0 prefetch-generator-1.0.3 protobuf-4.24.4 pydantic-1.10.13 pymultihash-0.8.2 safetensors-0.4.0 sentencepiece-0.1.99 speedtest-cli-2.1.3 tensor-parallel-1.0.23 tokenizers-0.14.1 transformers-4.34.0 uvloop-0.18.0 varint-1.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/bigscience-workshop/petals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0203bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7f00b6ab4e4da59add24fbb8ea3981"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'FloatProgress' object has no attribute 'style'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_41290/2199249825.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# You could also use \"meta-llama/Llama-2-70b-chat-hf\" or any other supported model from 🤗 Model Hub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_bos_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoDistributedModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m     resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    430\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1429\u001b[0m                     \u001b[0m_check_disk_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m             http_get(\n\u001b[0m\u001b[1;32m   1432\u001b[0m                 \u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m                 \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mdisplayed_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"(…){displayed_name[-20:]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m     progress = tqdm(\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0munit_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/tqdm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mare_progress_bars_disabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"disable\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__delattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplayed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolour\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;31m# Print initial bar state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mcolour\u001b[0;34m(self, bar_color)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcolour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbar_color\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'container'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar_color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbar_color\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FloatProgress' object has no attribute 'style'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from petals import AutoDistributedModelForCausalLM\n",
    "\n",
    "model_name = \"petals-team/StableBeluga2\"\n",
    "# You could also use \"meta-llama/Llama-2-70b-chat-hf\" or any other supported model from 🤗 Model Hub\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, add_bos_token=False)\n",
    "model = AutoDistributedModelForCausalLM.from_pretrained(model_name)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1df65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('In the sentence \"Recent work demonstrates the potential of multilingual pretraining of creating one model that can be used for various tasks in different languages. Previous work in multilingual pretraining has demonstrated that machine translation systems can be created by finetuning on bitext\", multilingual means: \"', return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
    "outputs = model.generate(inputs, max_new_tokens=3)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f6899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_token = tokenizer(\"^\")[\"input_ids\"][0]  # Workaround to make tokenizer.decode() keep leading spaces\n",
    "\n",
    "text = \"What is a good chatbot? Answer:\"\n",
    "prefix = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
    "\n",
    "with model.inference_session(max_length=30) as sess:\n",
    "    for i in range(20):\n",
    "        # Prefix is passed only for the 1st token of the outputs\n",
    "        inputs = prefix if i == 0 else None\n",
    "\n",
    "        # Let's use sampling with temperature = 0.9 and top_p = 0.6 to get more diverse results\n",
    "        outputs = model.generate(inputs, max_new_tokens=1, session=sess,\n",
    "                                 do_sample=True, temperature=0.9, top_p=0.6)\n",
    "\n",
    "        text += tokenizer.decode([fake_token, outputs[0, -1].item()])[1:]\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263a33b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model.inference_session(max_length=512) as sess:\n",
    "    while True:\n",
    "        prompt = input('Human: ')\n",
    "        if prompt == \"\":\n",
    "            break\n",
    "        prefix = f\"Human: {prompt}\\nFriendly AI:\"\n",
    "        prefix = tokenizer(prefix, return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
    "        print(\"Friendly AI:\", end=\"\", flush=True)\n",
    "\n",
    "        while True:\n",
    "            outputs = model.generate(prefix, max_new_tokens=1, session=sess,\n",
    "                                     do_sample=True, temperature=0.9, top_p=0.6)\n",
    "            outputs = tokenizer.decode([fake_token, outputs[0, -1].item()])[1:]\n",
    "\n",
    "            # Now, let's print one new token at a time\n",
    "            print(outputs, end=\"\", flush=True)\n",
    "\n",
    "            if \"\\n\" in outputs or \"</s>\" in outputs:\n",
    "                break\n",
    "            prefix = None  # Prefix is passed only for the 1st token of the bot's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfb5e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
